[build-system]
requires = ["setuptools>=68", "wheel"]
build-backend = "setuptools.build_meta"

[project]
name = "evaluating-slm"
version = "0.1.0"
description = "Scaffold for evaluating Small Language Models (SLMs)"
readme = "README.md"
requires-python = ">=3.9"
authors = [{ name = "Antonio Menta" }]
license = { text = "MIT" }
dependencies = []

[project.optional-dependencies]
# Core evaluation stack for comparing SLMs
eval = [
    "transformers>=4.43",
    "datasets>=2.20",
    "evaluate>=0.4",
    "accelerate>=0.30",
    "scikit-learn>=1.2",
    "numpy>=1.23",
    "pandas>=1.5",
    "tqdm>=4.64",
    "sentencepiece>=0.1.99",
    "sacrebleu>=2.4",
    "rouge-score>=0.1.2",
    # EleutherAI LM Evaluation Harness (optional but useful)
    "lm-eval>=0.4.2",
]

# Optional: API clients for hosted models
apis = [
    "openai>=1.40",
]

# Optional: PyTorch for local inference; install variant per platform as needed
torch = [
    "torch>=2.3",
]

# Optional: ONNX runtime for lightweight inference backends


# Dev tools (includes eval stack for convenience)
dev = [
    "pre-commit>=3.7",
    "ruff>=0.6",
    "black>=24.0",
    "pytest>=7.0",
    "ipykernel>=6.0",
    # evaluation stack
    "transformers>=4.43",
    "datasets>=2.20",
    "evaluate>=0.4",
    "accelerate>=0.30",
    "scikit-learn>=1.2",
    "numpy>=1.23",
    "pandas>=1.5",
    "tqdm>=4.64",
    "sentencepiece>=0.1.99",
    "sacrebleu>=2.4",
    "rouge-score>=0.1.2",
    "lm-eval>=0.4.2",
]

[project.scripts]
slm-eval = "evaluating_slm.cli:main"

[tool.black]
line-length = 100
target-version = ["py39"]

[tool.ruff]
line-length = 100
target-version = "py39"
extend-exclude = [
    "data",
]
lint.select = ["E", "F", "B", "I"]
lint.ignore = []

[tool.isort]
profile = "black"

[tool.pytest.ini_options]
addopts = "-q"
testpaths = ["tests"]

[tool.setuptools]
package-dir = {"" = "src"}

[tool.setuptools.packages.find]
where = ["src"]
include = ["evaluating_slm*"]
exclude = []
